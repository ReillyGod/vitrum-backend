{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a5b620a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "import praw\n",
    "import warnings\n",
    "\n",
    "from pmaw import PushshiftAPI\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a927ddc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>score</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>body</th>\n",
       "      <th>created</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Times Square right now</td>\n",
       "      <td>467860</td>\n",
       "      <td>l8rf4k</td>\n",
       "      <td>https://v.redd.it/x64z70f7eie61</td>\n",
       "      <td>13702</td>\n",
       "      <td></td>\n",
       "      <td>1.612030e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UPVOTE so everyone sees we got SUPPORT</td>\n",
       "      <td>331527</td>\n",
       "      <td>l6wu59</td>\n",
       "      <td>https://i.redd.it/sgoqy8nyt2e61.png</td>\n",
       "      <td>12932</td>\n",
       "      <td></td>\n",
       "      <td>1.611841e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GME YOLO update — Jan 28 2021</td>\n",
       "      <td>293629</td>\n",
       "      <td>l78uct</td>\n",
       "      <td>https://i.redd.it/opzucppb15e61.png</td>\n",
       "      <td>23298</td>\n",
       "      <td></td>\n",
       "      <td>1.611868e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GME YOLO month-end update — Jan 2021</td>\n",
       "      <td>260252</td>\n",
       "      <td>l846a1</td>\n",
       "      <td>https://i.redd.it/r557em3t5ce61.png</td>\n",
       "      <td>20232</td>\n",
       "      <td></td>\n",
       "      <td>1.611954e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It’s treason then</td>\n",
       "      <td>239236</td>\n",
       "      <td>l881ia</td>\n",
       "      <td>https://i.redd.it/d3t66lv1yce61.jpg</td>\n",
       "      <td>4643</td>\n",
       "      <td></td>\n",
       "      <td>1.611964e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title   score      id  \\\n",
       "0                  Times Square right now  467860  l8rf4k   \n",
       "1  UPVOTE so everyone sees we got SUPPORT  331527  l6wu59   \n",
       "2           GME YOLO update — Jan 28 2021  293629  l78uct   \n",
       "3    GME YOLO month-end update — Jan 2021  260252  l846a1   \n",
       "4                       It’s treason then  239236  l881ia   \n",
       "\n",
       "                                   url  num_comments body       created  \n",
       "0      https://v.redd.it/x64z70f7eie61         13702       1.612030e+09  \n",
       "1  https://i.redd.it/sgoqy8nyt2e61.png         12932       1.611841e+09  \n",
       "2  https://i.redd.it/opzucppb15e61.png         23298       1.611868e+09  \n",
       "3  https://i.redd.it/r557em3t5ce61.png         20232       1.611954e+09  \n",
       "4  https://i.redd.it/d3t66lv1yce61.jpg          4643       1.611964e+09  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scraping Reddit\n",
    "reddit = praw.Reddit(client_id='BIYwp3HhdUEh9_uOUVr0Tg', client_secret='JXTjLPONmrctnE6CCx5Z9BjYuaSXkw', user_agent='wsbScrape')\n",
    "\n",
    "#Collection of Initial DataSet\n",
    "posts = []\n",
    "wsbData = reddit.subreddit('WallStreetBets')\n",
    "\n",
    "#This fails around 100 posts. Not robust enough for large data collection.\n",
    "for post in wsbData.top(limit=10):\n",
    "    posts.append([post.title, post.score, post.id, post.url, post.num_comments, post.selftext, post.created])\n",
    "   \n",
    "dataSet = pd.DataFrame(posts,columns=['title', 'score', 'id', 'url', 'num_comments', 'body', 'created'])\n",
    "dataSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fd50db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 125053 posts from Pushshift\n"
     ]
    }
   ],
   "source": [
    "#This API works better for large data set collections and is multi threaded.\n",
    "api = PushshiftAPI()\n",
    "\n",
    "#Set the range and subreddit for data collection\n",
    "#In this case 1000000 posts from r/wallstreetbets January 1, 2021 to September 1, 2022.\n",
    "before = int(dt.datetime(2022,9,1,0,0).timestamp())\n",
    "after = int(dt.datetime(2021,1,1,0,0).timestamp())\n",
    "subreddit=\"wallstreetbets\"\n",
    "limit=1000000\n",
    "\n",
    "#Function to filter out posts with less than 10 upvotes.\n",
    "def fxn(item):\n",
    "  return item['score'] >= 10\n",
    "\n",
    "#Collect the posts and print total number when done.\n",
    "#Completed in 5 Hours 34 Minutes.\n",
    "api_praw = PushshiftAPI(praw=reddit, num_workers = 18)\n",
    "posts = api_praw.search_submissions(subreddit=subreddit, limit=limit, before=before, after=after, filter_fn=fxn)\n",
    "print(f'Retrieved {len(posts)} posts from Pushshift')\n",
    "\n",
    "#Save Example as CSV to inspect output format.\n",
    "postsDF = pd.DataFrame(posts)\n",
    "postsDF.to_csv('./wsb_posts.csv', header=True, index=False, columns=list(postsDF.axes[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "87e771d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>link_flair_css_class</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>date</th>\n",
       "      <th>permalink</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yolo loss waiting to picked up at the moon. Am...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yolo</td>\n",
       "      <td>14</td>\n",
       "      <td>0.83</td>\n",
       "      <td>16</td>\n",
       "      <td>2021-04-09 19:42:24+00:00</td>\n",
       "      <td>/r/wallstreetbets/comments/mnp084/yolo_loss_wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So, I didn’t tell the wife I used the equity f...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>yolo</td>\n",
       "      <td>7768</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1304</td>\n",
       "      <td>2021-04-09 19:39:53+00:00</td>\n",
       "      <td>/r/wallstreetbets/comments/mnoyga/so_i_didnt_t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>... Then You Haven't Met The Apes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>meme</td>\n",
       "      <td>70</td>\n",
       "      <td>0.88</td>\n",
       "      <td>19</td>\n",
       "      <td>2021-04-09 19:38:01+00:00</td>\n",
       "      <td>/r/wallstreetbets/comments/mnox4t/then_you_hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I know it's a difficult day for GME apes. I`m ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>meme</td>\n",
       "      <td>40</td>\n",
       "      <td>0.92</td>\n",
       "      <td>13</td>\n",
       "      <td>2021-04-09 19:36:04+00:00</td>\n",
       "      <td>/r/wallstreetbets/comments/mnovrc/i_know_its_a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You see this my fellow apes? ITS RISING KEEP T...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>profit</td>\n",
       "      <td>246</td>\n",
       "      <td>0.96</td>\n",
       "      <td>16</td>\n",
       "      <td>2021-02-03 18:36:23+00:00</td>\n",
       "      <td>/r/wallstreetbets/comments/lbuln3/you_see_this...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title   selftext  \\\n",
       "0  Yolo loss waiting to picked up at the moon. Am...        NaN   \n",
       "1  So, I didn’t tell the wife I used the equity f...  [deleted]   \n",
       "2               ... Then You Haven't Met The Apes...        NaN   \n",
       "3  I know it's a difficult day for GME apes. I`m ...        NaN   \n",
       "4  You see this my fellow apes? ITS RISING KEEP T...        NaN   \n",
       "\n",
       "  link_flair_css_class  score  upvote_ratio  num_comments  \\\n",
       "0                 yolo     14          0.83            16   \n",
       "1                 yolo   7768          0.81          1304   \n",
       "2                 meme     70          0.88            19   \n",
       "3                 meme     40          0.92            13   \n",
       "4               profit    246          0.96            16   \n",
       "\n",
       "                       date                                          permalink  \n",
       "0 2021-04-09 19:42:24+00:00  /r/wallstreetbets/comments/mnp084/yolo_loss_wa...  \n",
       "1 2021-04-09 19:39:53+00:00  /r/wallstreetbets/comments/mnoyga/so_i_didnt_t...  \n",
       "2 2021-04-09 19:38:01+00:00  /r/wallstreetbets/comments/mnox4t/then_you_hav...  \n",
       "3 2021-04-09 19:36:04+00:00  /r/wallstreetbets/comments/mnovrc/i_know_its_a...  \n",
       "4 2021-02-03 18:36:23+00:00  /r/wallstreetbets/comments/lbuln3/you_see_this...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I am running this after restarting Jupyter Notebook so I will read from the CSV file now.\n",
    "postsDF = pd.read_csv('wsb_posts.csv', header=0)\n",
    "\n",
    "#Convert UTC Epoch to standard date and time.\n",
    "postsDF['date'] = pd.to_datetime(postsDF['created_utc'], utc=True, unit='s')\n",
    "\n",
    "#Create new DataFrame only with info that is required to reduce file size.\n",
    "dataset = postsDF[['title', 'selftext', 'link_flair_css_class','score', 'upvote_ratio', 'num_comments', 'date','permalink']]\n",
    "#dataset.to_csv('./wsb_dataset.csv', header=True, index=False, columns=list(dataset.axes[1]))\n",
    "\n",
    "#Show the raw DataFrame.\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6d628773",
   "metadata": {},
   "outputs": [],
   "source": [
    "profit = dataset[dataset.link_flair_css_class == 'profit']\n",
    "loss = dataset[dataset.link_flair_css_class == 'loss']\n",
    "\n",
    "#We have 10869 posts tagged 'profit'.\n",
    "#These will make up the positive sentiment data set.\n",
    "#Now to create the positive sentiment data set.\n",
    "profit = profit[['title','link_flair_css_class']]\n",
    "profit = profit.rename(columns={'title':'text', 'link_flair_css_class' : 'label'})\n",
    "profit = profit.assign(label=1)\n",
    "profit.sample(7500).to_csv('./positive.csv', header=True, index=False, columns=list(profit.axes[1]))\n",
    "\n",
    "#We have 7924 posts tagged 'loss'.\n",
    "#These will make up the negative sentiment data set.\n",
    "loss = loss[['title','link_flair_css_class']]\n",
    "loss = loss.rename(columns={'title':'text', 'link_flair_css_class' : 'label'})\n",
    "loss = loss.assign(label=0)\n",
    "loss.sample(7500).to_csv('./negative.csv', header=True, index=False, columns=list(loss.axes[1]))\n",
    "\n",
    "#A validation data set containing 500 random posts will be selected from the remaining posts\n",
    "#I will classify this data myself\n",
    "test = dataset[dataset.score >=1000]\n",
    "test = test.drop(test.loc[test['link_flair_css_class']=='profit'].index)\n",
    "test = test.drop(test.loc[test['link_flair_css_class']=='loss'].index)\n",
    "test = test.drop(test.loc[test['link_flair_css_class']==''].index)\n",
    "test = test.drop(test.loc[test['title']=='[deleted by user]'].index)\n",
    "test = test[['title','link_flair_css_class']]\n",
    "test = test.rename(columns={'title':'text', 'link_flair_css_class' : 'label'})\n",
    "test = test.assign(label=1)\n",
    "test.sample(500).to_csv('./test.csv', header=True, index=False, columns=list(test.axes[1]))\n",
    "\n",
    "#The Data will now be checked over to ensure proper formating as some post titles cause issues with CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6f22df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7809931090712c0e\n",
      "Reusing dataset csv (/home/reilly/.cache/huggingface/datasets/csv/default-7809931090712c0e/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0fe7321821f48039792b2bc48a03257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/reilly/.cache/huggingface/datasets/csv/default-7809931090712c0e/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a/cache-618203d2865f6021.arrow\n",
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "from datasets import load_dataset, Dataset, load_metric\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer as at\n",
    "from transformers import DataCollatorWithPadding as dcwp\n",
    "from transformers import AutoModelForSequenceClassification as amfsc\n",
    "from transformers import DataCollatorWithPadding as dcwp\n",
    "\n",
    "#Compute the accuacy of the model once completed training.\n",
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\")\n",
    "   load_f1 = load_metric(\"f1\")\n",
    "  \n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}\n",
    "\n",
    "#The function that tokenizes the text.\n",
    "def encode(data):\n",
    "    return tokenizer(data[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "#Load and split the data sets.\n",
    "dataset = load_dataset('csv', data_files=['positive_clean.csv', 'negative_clean.csv'])['train'].shuffle(seed=41).train_test_split(test_size=0.2)\n",
    "\n",
    "#Specify the model to be used for fine tuning.\n",
    "model_name = 'roberta-large'\n",
    "model = amfsc.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = at.from_pretrained(model_name)\n",
    "data_collator = dcwp(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53f9fef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "199fb6c9d59048498d36106620755c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d28b3e1567240f399ea037731496694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Tokenize the data using a map function to speed up the process.\n",
    "token_train = dataset['train'].map(encode, batched=True)\n",
    "token_test = dataset['test'].map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cf3435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting the Paramaters for fine tunning.\n",
    "training_args = TrainingArguments(\n",
    "   output_dir='roberta15000',\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=14,\n",
    "   per_device_eval_batch_size=14,\n",
    "   num_train_epochs=12,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=token_train,\n",
    "   eval_dataset=token_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e80e654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 12000\n",
      "  Num Epochs = 12\n",
      "  Instantaneous batch size per device = 14\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 28\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5148\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5148' max='5148' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5148/5148 1:05:51, Epoch 12/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.546300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.396400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.315800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.232500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.179800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.142700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.110600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.093600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.066700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.058000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test_15000_samples/checkpoint-429\n",
      "Configuration saved in test_15000_samples/checkpoint-429/config.json\n",
      "Model weights saved in test_15000_samples/checkpoint-429/pytorch_model.bin\n",
      "tokenizer config file saved in test_15000_samples/checkpoint-429/tokenizer_config.json\n",
      "Special tokens file saved in test_15000_samples/checkpoint-429/special_tokens_map.json\n",
      "Saving model checkpoint to test_15000_samples/checkpoint-858\n",
      "Configuration saved in test_15000_samples/checkpoint-858/config.json\n",
      "Model weights saved in test_15000_samples/checkpoint-858/pytorch_model.bin\n",
      "tokenizer config file saved in test_15000_samples/checkpoint-858/tokenizer_config.json\n",
      "Special tokens file saved in test_15000_samples/checkpoint-858/special_tokens_map.json\n",
      "Saving model checkpoint to test_15000_samples/checkpoint-1287\n",
      "Configuration saved in test_15000_samples/checkpoint-1287/config.json\n",
      "Model weights saved in test_15000_samples/checkpoint-1287/pytorch_model.bin\n",
      "tokenizer config file saved in test_15000_samples/checkpoint-1287/tokenizer_config.json\n",
      "Special tokens file saved in test_15000_samples/checkpoint-1287/special_tokens_map.json\n",
      "Saving model checkpoint to test_15000_samples/checkpoint-1716\n",
      "Configuration saved in test_15000_samples/checkpoint-1716/config.json\n",
      "Model weights saved in test_15000_samples/checkpoint-1716/pytorch_model.bin\n",
      "tokenizer config file saved in test_15000_samples/checkpoint-1716/tokenizer_config.json\n",
      "Special tokens file saved in test_15000_samples/checkpoint-1716/special_tokens_map.json\n",
      "Saving model checkpoint to test_15000_samples/checkpoint-2145\n",
      "Configuration saved in test_15000_samples/checkpoint-2145/config.json\n",
      "Model weights saved in test_15000_samples/checkpoint-2145/pytorch_model.bin\n",
      "tokenizer config file saved in test_15000_samples/checkpoint-2145/tokenizer_config.json\n",
      "Special tokens file saved in test_15000_samples/checkpoint-2145/special_tokens_map.json\n",
      "Saving model checkpoint to test_15000_samples/checkpoint-2574\n",
      "Configuration saved in test_15000_samples/checkpoint-2574/config.json\n",
      "Model weights saved in test_15000_samples/checkpoint-2574/pytorch_model.bin\n",
      "tokenizer config file saved in test_15000_samples/checkpoint-2574/tokenizer_config.json\n",
      "Special tokens file saved in test_15000_samples/checkpoint-2574/special_tokens_map.json\n",
      "Saving model checkpoint to test_15000_samples/checkpoint-3003\n",
      "Configuration saved in test_15000_samples/checkpoint-3003/config.json\n",
      "Model weights saved in test_15000_samples/checkpoint-3003/pytorch_model.bin\n",
      "tokenizer config file saved in test_15000_samples/checkpoint-3003/tokenizer_config.json\n",
      "Special tokens file saved in test_15000_samples/checkpoint-3003/special_tokens_map.json\n",
      "Saving model checkpoint to test_15000_samples/checkpoint-3432\n",
      "Configuration saved in test_15000_samples/checkpoint-3432/config.json\n",
      "Model weights saved in test_15000_samples/checkpoint-3432/pytorch_model.bin\n",
      "tokenizer config file saved in test_15000_samples/checkpoint-3432/tokenizer_config.json\n",
      "Special tokens file saved in test_15000_samples/checkpoint-3432/special_tokens_map.json\n",
      "Saving model checkpoint to test_15000_samples/checkpoint-3861\n",
      "Configuration saved in test_15000_samples/checkpoint-3861/config.json\n",
      "Model weights saved in test_15000_samples/checkpoint-3861/pytorch_model.bin\n",
      "tokenizer config file saved in test_15000_samples/checkpoint-3861/tokenizer_config.json\n",
      "Special tokens file saved in test_15000_samples/checkpoint-3861/special_tokens_map.json\n",
      "Saving model checkpoint to test_15000_samples/checkpoint-4290\n",
      "Configuration saved in test_15000_samples/checkpoint-4290/config.json\n",
      "Model weights saved in test_15000_samples/checkpoint-4290/pytorch_model.bin\n",
      "tokenizer config file saved in test_15000_samples/checkpoint-4290/tokenizer_config.json\n",
      "Special tokens file saved in test_15000_samples/checkpoint-4290/special_tokens_map.json\n",
      "Saving model checkpoint to test_15000_samples/checkpoint-4719\n",
      "Configuration saved in test_15000_samples/checkpoint-4719/config.json\n",
      "Model weights saved in test_15000_samples/checkpoint-4719/pytorch_model.bin\n",
      "tokenizer config file saved in test_15000_samples/checkpoint-4719/tokenizer_config.json\n",
      "Special tokens file saved in test_15000_samples/checkpoint-4719/special_tokens_map.json\n",
      "Saving model checkpoint to test_15000_samples/checkpoint-5148\n",
      "Configuration saved in test_15000_samples/checkpoint-5148/config.json\n",
      "Model weights saved in test_15000_samples/checkpoint-5148/pytorch_model.bin\n",
      "tokenizer config file saved in test_15000_samples/checkpoint-5148/tokenizer_config.json\n",
      "Special tokens file saved in test_15000_samples/checkpoint-5148/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5148, training_loss=0.20953205452469215, metrics={'train_runtime': 3954.152, 'train_samples_per_second': 36.417, 'train_steps_per_second': 1.302, 'total_flos': 9.301279305256848e+16, 'train_loss': 0.20953205452469215, 'epoch': 12.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model. Took roughly 1 hour with two rtx 3090's\n",
    "#Had trouble fitting the models in VRAM. It was close and dependant on other system processes.\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "020e2326",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3000\n",
      "  Batch size = 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.2885855436325073,\n",
       " 'eval_accuracy': 0.81,\n",
       " 'eval_f1': 0.8105053191489362,\n",
       " 'eval_runtime': 14.8971,\n",
       " 'eval_samples_per_second': 201.382,\n",
       " 'eval_steps_per_second': 7.25,\n",
       " 'epoch': 12.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate the model using the evaluate function\n",
    "#Initial performace on distil-bert and bert was lower at 0.76 accuracy and f1 score\n",
    "#roBERTa preformed approximatley 5% better at .81 accuracy and f1 score, this is on the low side of accceptable.\n",
    "#The f1 score is a good metric to use as it accounts for false positives in its calculations.\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ebce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I will continue to explore using other models on my collected data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559225d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
